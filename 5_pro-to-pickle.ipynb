{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pickle files from .pro files\n",
    "\n",
    "#### Created by Megan Thompson-Munson (2023)\n",
    "\n",
    "**Input:** \n",
    "- `/scratch/alpine/metm9666/project-2_output-4/*.pro`\n",
    "\n",
    "**Output:** \n",
    "- `/scratch/alpine/metm9666/project-2_processed-output-4/*firn-data.p`\n",
    "- `/scratch/alpine/metm9666/project-2_processed-output-4/*firn-data-interpolated.p`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import glob\n",
    "from scipy import stats\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.interpolate import griddata\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path\n",
    "path_raw = '/scratch/alpine/metm9666/project-2_output-4/'\n",
    "path_processed = '/scratch/alpine/metm9666/project-2_processed-output-4/pro-files_processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for reading in and processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read proFile\n",
    "def read_pro(proFile):\n",
    "\n",
    "    # Read data line by line\n",
    "    with open(proFile) as f:\n",
    "\n",
    "        # Read in header information\n",
    "        for h in range(39):\n",
    "            header = f.readline()\n",
    "            if h == 1:\n",
    "                station = int(header[13:])\n",
    "            if h == 2:\n",
    "                lats = (float(header[10:-1]))\n",
    "            if h == 3:\n",
    "                lons = (float(header[11:-1]))\n",
    "\n",
    "        # Create empty lists for each variable\n",
    "        date = []\n",
    "        depth = []\n",
    "        thickness = []\n",
    "        density = []\n",
    "        temperature = []\n",
    "        age = []\n",
    "        water = []\n",
    "        ice = []\n",
    "        air = []\n",
    "\n",
    "        # Read in data below header\n",
    "        for line in f:\n",
    "            if line.startswith('0500'): # Date\n",
    "                date.append(datetime.datetime.strptime(line[5:24],'%d.%m.%Y %H:%M:%S'))\n",
    "            if line.startswith('0501'): # Height\n",
    "                heightArray = np.array(list(map(float,line.split(',')))[2:])/100\n",
    "                depthArray = (heightArray-heightArray[-1])*-1\n",
    "                depth.append(depthArray)\n",
    "                thicknessShort = np.array(heightArray[1:]-heightArray[:-1])\n",
    "                thicknessArray = np.insert(thicknessShort,0,heightArray[0])\n",
    "                thickness.append(thicknessArray)\n",
    "            if line.startswith('0502'): # Density\n",
    "                density.append(np.array(list(map(float,line.split(',')))[2:]))\n",
    "            if line.startswith('0503'): # Temperature\n",
    "                temperature.append(np.array(list(map(float,line.split(',')))[2:]))         \n",
    "            if line.startswith('0506'): # Ice\n",
    "                water.append(np.array(list(map(float,line.split(',')))[2:])/100)  \n",
    "            if line.startswith('0515'): # Water\n",
    "                ice.append(np.array(list(map(float,line.split(',')))[2:])/100)                  \n",
    "            if line.startswith('0516'): # Air\n",
    "                air.append(np.array(list(map(float,line.split(',')))[2:])/100)\n",
    "\n",
    "    # Close file\n",
    "    f.close()\n",
    "\n",
    "    fac = []\n",
    "    for t in range(len(date)):\n",
    "        fac.append(np.sum(thickness[t]*air[t]))\n",
    "        \n",
    "    data = {'date':date,'station':station,'lat':lats,'lon':lons,'fac':fac,\n",
    "            'fac_smooth':savgol_filter(fac,501,2),\n",
    "            'depth':depth,'thickness':thickness,'density':density,\n",
    "            'temperature':temperature,'water':water,'ice':ice,'air':air}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interpolate onto regular vertical grid\n",
    "def interpolate(pro,variable,maxdepth):\n",
    "    \n",
    "    t200 = np.linspace(0,200,len(pro['date']))\n",
    "    \n",
    "    if maxdepth<=10:\n",
    "        z = np.arange(0,maxdepth+0.01,0.01)[::-1]\n",
    "    else:\n",
    "        z = np.concatenate([np.arange(0,10.01,0.01),np.arange(11,maxdepth+1,1)])[::-1]\n",
    "        \n",
    "    y_interp = []\n",
    "    for i in range(len(t200)):\n",
    "        zi = pro['depth'][i]\n",
    "        yi = pro[variable][i][zi<=maxdepth]\n",
    "        zi = zi[zi<=maxdepth]\n",
    "        \n",
    "        y_interp.append(griddata(zi,yi,z))\n",
    "        \n",
    "    return pd.DataFrame(data=np.stack(y_interp).T,index=z,columns=t200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_timeseries(dec_pro,inc_pro):\n",
    "    \n",
    "    date_array = np.array(dec_pro['date'])\n",
    "    keys = np.array(['date','fac','fac_smooth','depth',\n",
    "                     'thickness','density','temperature','water','ice','air'])\n",
    "\n",
    "    # Split data into initial (0-100), dec (100-200), and inc (100-200)\n",
    "    initial = {}\n",
    "    dec = {}\n",
    "    inc = {}\n",
    "    \n",
    "    for i in range(len(keys)):\n",
    "\n",
    "        # Convert to arrays\n",
    "        dec_array = np.array(dec_pro[keys[i]])\n",
    "        inc_array = np.array(inc_pro[keys[i]])\n",
    "\n",
    "        # Add each array as new key to each dictionary\n",
    "        initial[keys[i]] = dec_array[date_array<pd.to_datetime('2080-01-01 00:30:00')]\n",
    "        dec[keys[i]] = dec_array[date_array>=pd.to_datetime('2080-01-01 00:30:00')]\n",
    "        inc[keys[i]] = inc_array[date_array>=pd.to_datetime('2080-01-01 00:30:00')]\n",
    "        \n",
    "    # Add time as years\n",
    "    initial['years'] = np.linspace(0,100,2610,endpoint='False')\n",
    "    dec['years'] = np.linspace(100,200,2609)\n",
    "    inc['years'] = np.linspace(100,200,2609)\n",
    "    \n",
    "    # Crate new dictionary to wrap all others and add in lat/lon\n",
    "    data = {'station':dec_pro['station'],'lat':dec_pro['lat'],'lon':dec_pro['lon'],\n",
    "            'initial':initial,'dec':dec,'inc':inc}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_interpolation(dec_pro,inc_pro):\n",
    "    \n",
    "    # Interpolate density data\n",
    "    rho_dec = interpolate(dec_pro,'density',100)\n",
    "    rho_inc = interpolate(inc_pro,'density',100)\n",
    "\n",
    "    # Zoom in on time/depths of interest (go slightly wider than needed)\n",
    "    rho_dec_zoom = rho_dec.loc[31:0,89:111]\n",
    "    rho_inc_zoom = rho_inc.loc[31:0,89:111]\n",
    "\n",
    "    # Interpolate firn temperature data\n",
    "    ft_dec = interpolate(dec_pro,'temperature',100)\n",
    "    ft_inc = interpolate(inc_pro,'temperature',100)\n",
    "\n",
    "    # Zoom in on time/depths of interest\n",
    "    ft_dec_zoom = ft_dec.loc[31:0,89:111]\n",
    "    ft_inc_zoom = ft_inc.loc[31:0,89:111]\n",
    "\n",
    "    # Interpolate water content data\n",
    "    water_dec = interpolate(dec_pro,'water',100)\n",
    "    water_inc = interpolate(inc_pro,'water',100)\n",
    "\n",
    "    # Zoom in on time/depths of interest\n",
    "    water_dec_zoom = water_dec.loc[31:0,89:111]\n",
    "    water_inc_zoom = water_inc.loc[31:0,89:111]\n",
    "    \n",
    "    return {'station':dec_pro['station'],'lat':dec_pro['lat'],'lon':dec_pro['lon'],\n",
    "            'rho_dec':rho_dec_zoom,'rho_inc':rho_inc_zoom,\n",
    "            'temp_dec':ft_dec_zoom,'temp_inc':ft_inc_zoom,\n",
    "            'lwc_dec':water_dec_zoom,'lwc_inc':water_inc_zoom}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cooling > warming (cooling eliminates melt): 636, 67\n",
    "\n",
    "# Warming slightly > cooling (no melt): 1763, 1791, 1085\n",
    "\n",
    "# Warming > cooling (warming creates melt): 549, 1542\n",
    "\n",
    "# Warming > cooling (mean summer temp < 0, but melt >0): 1541, 619, 683\n",
    "\n",
    "# Cooling > warming (warming depletes firn): 1026, 859\n",
    "\n",
    "# Cooling > warming (Summer temp >0 and initial FAC ~0): 1718, 993"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/scratch/alpine/metm9666/project-2_processed-output-4/FAC-change_vs_atm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 993,  942,  372, 1718,  448, 1930,  881,  649,   81, 1199, 1737,\n",
       "       1581, 1074, 1276, 1677, 1478, 1942, 1666,  739,  212,  595, 1600,\n",
       "       1593, 1077, 1622, 1869, 1156, 1890, 1087, 1303,  678,  319, 1812,\n",
       "       1766,  144,  846, 1344,  288,   52,  734,  442,  331,  717,  936,\n",
       "        402,  675, 1702,  693,    6,  184,  472, 1565,  277, 1346, 1575,\n",
       "       1157,  267,  156,  534,  633, 1698,  606, 1160,  892, 1283,  499,\n",
       "       1351, 1903])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[df['melt']>0][df['melt_cool']==0].sort_values(by=['melt'],ascending=False)\n",
    "# df[df['melt']>0][df['melt_cool']==0][df['melt_warm']>=df['melt']].sort_values(by=['melt'],ascending=False).idx.values\n",
    "# df[df['melt_warm']==0][df['summer_temp']<-10][df['warm_change']<=-2]\n",
    "# df[df['melt']==0][df['melt_warm']>0].sort_values(by=['melt_warm'],ascending=False).iloc[0:10].idx.values\n",
    "# df[df['summer_temp']>-6][df['summer_temp']<-4][df['cool_warm_diff']<0][df['melt']>0]\n",
    "# df[df['summer_temp']>-3][df['summer_temp']<0][df['cool_warm_diff']>0][df['melt']>300].idx.values\n",
    "df[df['melt']>700][df['summer_temp']>0].idx.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply functions and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = [1763,1542,683,859,232]\n",
    "f = [993,  942,  372, 1718,  448, 1930,  881,  649,   81, 1199, 1737]\n",
    "files = [[path_raw+'{}_TA_DEC_REPEAT1991.pro'.format(i),path_raw+'{}_TA_INC_REPEAT1991.pro'.format(i)] for i in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through, read, process, and export\n",
    "for i in range(len(files)):\n",
    "    pro_dec = read_pro(files[i][0])\n",
    "    pro_inc = read_pro(files[i][1])\n",
    "\n",
    "    data_ts = split_timeseries(pro_dec,pro_inc)\n",
    "    pickle.dump(data_ts,\n",
    "                open(path_processed+'{}_firn-data.p'.format(pro_dec['station']),'wb'))\n",
    "\n",
    "    data_interp = apply_interpolation(pro_dec,pro_inc)\n",
    "    pickle.dump(data_interp,\n",
    "                open(path_processed+'{}_firn-data-interpolated.p'.format(pro_dec['station']),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gather all files and read in using function\n",
    "# files = sorted(glob.glob(path_raw+'*.pro'))\n",
    "\n",
    "# # Loop through, read, process, and export\n",
    "# for i in np.arange(0,len(files),2):\n",
    "    \n",
    "#     if i!=136:\n",
    "#         pro_dec = read_pro(files[i])\n",
    "#         pro_inc = read_pro(files[i+1])\n",
    "\n",
    "#         data_ts = split_timeseries(pro_dec,pro_inc)\n",
    "#         pickle.dump(data_ts,\n",
    "#                     open(path_processed+'{}_firn-data.p'.format(pro_dec['station']),'wb'))\n",
    "\n",
    "#         data_interp = apply_interpolation(pro_dec,pro_inc)\n",
    "#         pickle.dump(data_interp,\n",
    "#                     open(path_processed+'{}_firn-data-interpolated.p'.format(pro_dec['station']),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpack",
   "language": "python",
   "name": "snowpack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
